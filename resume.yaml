cv:
  name: "Subhayu Kumar Bala"
  location: "Kolkata, India"
  email: "balasubhayu99@gmail.com"
  phone: "tel:+919382877751"
  website: "https://subhayu99.github.io"
  resume_url: "https://subhayu99.github.io/resume.pdf"
  social_networks:
    - network: "LinkedIn"
      username: "subhayu-kumar-bala"
    - network: "GitHub"
      username: "subhayu99"
    # - network: "ORCID"
    #   username: "0000-0001-8195-3118"
  sections:
    intro:
      - "Data and infrastructure engineer with 4 years of experience bridging traditional data engineering with modern AI systems, specializing in Python, SQL, and cloud platforms (AWS/Azure/GCP). Expertise in data architecture, warehousing, data lakes, and governance."
      - "Proven track record of exceptional performance optimization (reducing a financial procedure **from 27.5h to under 5s**), building production **Agentic LLM systems (>95% accuracy) on H100 clusters**, and authoring **open-source tools with 27,000+ PyPI downloads**."
      - "Delivered measurable business outcomes: data infrastructure enabling a startup's **$6M Series A**, unblocking **investor audits for Advent International**, ensuring enterprise compliance for a Fortune 500 client, and automating workflows to save **10-15 FTEs**."
    technologies:
      - label: "Data Engineering"
        details: "SQL, Databricks, BigQuery, MS Fabric, PySpark, DuckDB, DBT, Airflow, Kafka, ADF, Delta Lake, Pandas, Presto/Trino"
      - label: "AI & LLM Ops"
        details: "OpenAI, Gemini, LangChain, RAG, Agentic AI (MCP, A2A), Fine-tuning (SFT/DPO), Vector DBs (Chroma/Qdrant), Neo4j"
      - label: "Backend, DevOps & Viz"
        details: "Python, BASH, Git, FastAPI, PostgreSQL, MongoDB, AWS, Azure, GCP, Docker, K8s, Terraform, CI/CD, Power BI"
    experience:
    - company: "FiftyFive Technologies"
      position: "Data Engineer"
      location: "Gurugram, HR (Remote)"
      start_date: "2022-06"
      summary: "Delivered end-to-end data and AI solutions for 10+ enterprise clients across healthcare, finance, logistics, and tech sectors. Responsibilities extended beyond technical delivery: conducted 100+ technical interviews, led client onboarding and scoping calls, created PoCs/demos, mentored teammates, and drove adoption of emerging technologies (DuckDB, MCP) across the organization.\\ **Client Engagements:**"
      highlights:
        - "**SBGC Group (Retail Data Platform)**: Took ownership of a multi-region Azure/Databricks platform integrating SAP/Shopify/Klaviyo/GA4. **Unblocked Advent International due diligence** by reverse-engineering Shopify's logic to **cut revenue variance from ~10% to \\<0.22%**. Root-caused a critical legacy data explosion (**slashing volume by 99%**) and delivered an **inventory dashboard worth ~$250k**."
        - "**AGY Logistics (IoT & Automation)**: Sole architect of a GDP-compliant cold chain system on GCP (BigQuery, Cloud Run). Automated complex manual workflows previously requiring **10-15 US-based operations staff**, resulting in massive OpEx savings. Engineered smart time-series merging and statistical analysis on a 4-layer medallion architecture (including a 'Diamond Layer' for sub-second alerts) to **reduce false positives by 70%**."
        - "**Johnson & Johnson (Healthcare CDP)**: Architected scalable Treasure Data pipelines for regulated healthcare markets (JP/ANZ). Engineered complex transformation logic, including a 12-scenario SQL truth table (validated via TDD) to resolve hierarchical consent conflicts and a custom Python engine to overcome SFMC platform limitations, ensuring strict **GDPR/PII compliance**."
        - "**Wade Insight (Cloud Migration)**: Led enterprise migration from Azure Data Factory to **Microsoft Fabric Data Factory**, managing ARM template adaptation. Enhanced the SaaS platform with advanced continue-on-failure orchestration and automated health-check processes, reducing manual troubleshooting by 80%."
        - "**Prospexs (GenAI Product Engineering)**: Built an AI outreach platform (Python/FastAPI/MongoDB) backed by a **700M+ contact database**. Engineered a complex 4-entity personalization engine (Sender/Receiver × Human/Company) using OpenAI/Perplexity to generate bilingual, context-aware communications, **improving response rates by 45%**."
        - "**QxLab (LLM Infrastructure)**: Led a 4-person team fine-tuning Llama/Mistral models using Axolotl on **H100 clusters** to achieve **\\>95% accuracy** in a production DAG-based agentic system. Separately, architected a Terabyte-scale cascading deduplication pipeline (MinHash LSH + FAISS) to standardize datasets (SFT/DPO/RLHF). Later open-sourced as **DatasetPipeline (6.7k+ downloads)**."
        - "**CV Advisors (Performance Engineering)**: Replaced a 1900-line MSSQL procedure (iterative cursors) into vectorized logic (**DuckDB**/Pandas), processing 80M+ records across 150 clients. **Reduced runtime from 27.5h to \\<5s (\\>99.9% gain)**. Engineered a future-proof architecture capable of scaling beyond RAM limits via memory-optimized categorical typing and DuckDB’s out-of-core processing."
        # - "**Logical Contract (Legal Tech)**: Implemented an AI-powered legal tech system for generating tailored employment agreements and a legal chatbot for startup inquiries."
        - "**SlideNinja (GenAI RAG Architecture)**: Developed a GenAI RAG platform (LangChain/ChromaDB) for a **McKinsey partner** during the early GenAI boom, featuring a 'Self-Healing' AI orchestration layer and proprietary 'Geometric Layout Analysis' to map unstructured content into rigid corporate PowerPoint templates."
        - "**LoopKitchen (Real-time Data Ingestion)**: Built core data architecture (GCP/BigQuery/FastAPI/GCP Workflows) enabling a **$6M Series A**. Migrated fragile legacy scrapers to robust official API integrations (UberEats/DoorDash/Grubhub) for millions of orders and engineered an automated dispute resolution system to directly recover lost revenue."

    - company: "FiftyFive Technologies"
      position: "Software Engineer Intern"
      location: "Gurugram, HR (Remote)"
      start_date: "2022-01"
      end_date: "2022-05"
      summary: "Gained experience with production infrastructure (Docker, Terraform, K8s, CI/CD), backend best practices, and client-facing delivery.\\ **Client Engagements:**"
      highlights:
        - "**Eningo**: Developed a cloud-based geospatial platform; migrated legacy data from Postgres to MongoDB for enhanced scalability."
        - "**NIBE**: Managed and established Azure DevOps CI/CD pipelines for critical services supporting 50,000+ daily users."
    education:
      - institution: "Calcutta Institute of Engineering and Management (MAKAUT)"
        location: "Kolkata, WB"
        area: "Information Technology"
        degree: "B.Tech"
        start_date: "2018-07"
        end_date: "2022-06"
        highlights:
          - "**CGPA:** 8.57/10 ([B.Tech Certificate](https://drive.google.com/file/d/1xX8XtAlEEnXSgkcJ3jYmYSJOQWRylA6S/view))"
          - "**Google DSC Lead (2020-2021):** Led developer community, organized workshops and tech talks. [Google Dev Profile](https://g.dev/subhayu99)"
          - "**CodeChef Chapter Lead (2020-2021):** Hosted 4 competitive programming events (4 to 21 students). [CodeChef Certificate](https://drive.google.com/file/d/1NYokFweEGTAfgL3sZO6qeEiyC6HD9pQ7/view)"
          - "**Linux Foundation Scholarship Recipient:** Merit selection for LFS201 Linux System Administration. [Credly Profile](https://www.credly.com/users/subhayu99/badges)"
    professional_projects:
      # Website-only projects (show_on_resume: false)
      - name: "SBGC Group - Multi-Region Retail Data Platform"
        date: "Jul 2025 – Present"
        show_on_resume: false
        highlights:
          - "Took ownership of a multi-region Azure/Databricks Medallion architecture integrating SAP, Shopify, Klaviyo, and GA4. Unblocked Advent International due diligence by reverse-engineering Shopify's 'Net Sales' logic to exclude specific shipping refunds, reducing US revenue variance from ~10% to \\<0.22%."
          - "Resolved a critical 7-day infrastructure crisis where Azure VM capacity constraints halted production. Diagnosed cluster provisioning failures via logs and reconfigured Databricks node types (Standard_D4ads_v5) to restore all pipelines."
          - "Root-caused a critical data explosion bug (Cartesian join) that ballooned dataset size to 94M rows. Optimized the pipeline logic to slash volume by 99% (to 839k rows), significantly reducing compute costs."
          - "Executed a comprehensive Data Privacy compliance project (GDPR), implementing consent tracking across Shopify/SAP/Klaviyo and building PII-hashing strategies for investor data extracts."
          - "Built a cross-system 'Order Fulfillment Dashboard' integrating data from Shopify (Sales), SAP (ERP), and PDR (Logistics) to replace manual Excel reporting and track order SLAs in real-time."
      - name: "AGY Logistics - Reefer Temperature Monitoring System"
        date: "Jun 2025 – Sep 2025"
        show_on_resume: false
        highlights:
          - "Architected a GDP-compliant cold chain monitoring system on GCP using a Medallion Architecture (Bronze/Silver/Gold/Diamond). The 'Diamond Layer' pre-aggregated alert candidates, enabling the Alerting Engine to dispatch notifications with sub-second latency."
          - "Engineered 'Smart Merge' logic using SQL window functions (`LAST_VALUE`) to unify asynchronous data streams (sporadic TMS updates vs. high-frequency IoT telemetry) into a continuous, context-aware timeline."
          - "Designed an internal 'Phone Book' architecture to decouple sensor mapping from external TMS errors, ensuring data integrity even when manual entry errors occurred in the source system."
          - "Automated complex manual workflows previously requiring 10-15 US-based operations staff, resulting in massive OpEx savings."
          - "Slashed cloud compute costs by deprecating expensive BQML (Machine Learning) models in favor of optimized SQL-based statistical analysis (rolling windows/standard deviations), reducing false positive alerts by 70%."
      - name: "Johnson & Johnson - Healthcare Data Platform (CDP)"
        date: "Feb 2025 – Jun 2025"
        show_on_resume: false
        highlights:
          - "Architected a region-wide Customer Data Platform (CDP) for APAC (ANZ/JP) on Treasure Data, using a Medallion Architecture (Bronze/Silver/Gold) to unify Veeva CRM, SFMC, and GA4 data."
          - "Built a custom Python **SFMC Activation Engine** to overcome platform limitations (standard connectors cast all types to Text). Implemented smart schema inference and parallel processing (`ThreadPoolExecutor`) to push typed data (Booleans/Dates) to Salesforce Marketing Cloud via SOAP/REST APIs."
          - "Engineered the 'Golden Record' consent logic using a deterministic **12-scenario SQL Truth Table** to resolve conflicts between Global vs. Channel-specific opt-ins. Implemented **Test-Driven Development (TDD) in SQL** by creating a mock-data test suite to mathematically prove logic correctness for legal compliance."
          - "Solved a Big Data transformation challenge by writing complex SQL (`UNNEST`/`CROSS JOIN`) to flatten deeply nested, semi-structured **Google Analytics 4 (GA4)** JSON event logs into relational behavioral metrics for segmentation."
          - "Developed a **'Segmentation-as-Code'** CLI tool using Python/Typer. This allowed the team to manage complex Audience Studio configurations via version-controlled YAML templates, replacing error-prone GUI manual work."
          - "Implemented an **Automated Data Integrity Framework** that runs pre-ETL validation checks against defined schema contracts and business rules (e.g., 'Null % thresholds'), preventing silent data failures."
      - name: "Wade Insight - Cloud-Native Data Orchestration Platform"
        date: "Nov 2024 – Feb 2025"
        show_on_resume: false
        highlights:
          - "Enhanced core pipeline orchestration features for WADE's Azure SaaS platform, implementing comprehensive continue_on_failure mechanisms with SQL dependency handling and advanced job execution tracking."
          - "Led migration of enterprise data pipelines from Azure Data Factory to Microsoft Fabric Data Factory, managing ARM template adaptation and deployment for clients processing millions of records."
          - "Developed automated healthcheck processes leveraging Azure APIs to monitor running pipelines, reducing manual troubleshooting overhead by 80% and enabling faster insight delivery."
      - name: "Prospexs - GenAI Outreach Platform"
        date: "Jul 2024 – Oct 2024"
        show_on_resume: false
        highlights:
          - "Built the backend for an AI-powered outreach platform using Python/FastAPI and MongoDB, reducing manual prospecting efforts by 60% through intelligent automation."
          - "Developed a 'Website Analysis' module using scraping and prompt engineering to extract USPs and generate detailed business profiles automatically from raw company URLs."
          - "Engineered a complex 4-entity personalization engine that cross-analyzes distinct profiles (Sender Human/Company vs. Receiver Human/Company) to generate hyper-personalized, context-aware emails."
          - "Implemented bilingual generation capabilities (English/Swedish) with dynamic tone adjustment, integrating OpenAI/Perplexity/LinkedIn APIs to validate profiles and improve client response rates by 45%."
          - "Designed a fully automated pipeline using Cloud Scheduler for real-time data processing and model orchestration."
      - name: "QxLab - Agentic AI Infrastructure & LLMOps"
        date: "Jan 2024 – Jun 2024"
        show_on_resume: false
        highlights:
          - "Led a 4-person team fine-tuning Llama/Mistral models on H100 clusters to achieve **\\>95% accuracy** in a production-ready 'Mixture of Experts' agentic framework (Router/Planner/Extractor)."
          - "Architected the system's **DAG-based execution engine** (`StepRunner`) with dynamic state injection (Regex-based variable substitution) to handle complex, multi-step reasoning tasks."
          - "Solved a critical data engineering bottleneck where disparate, ad-hoc scripts failed to scale for Terabyte-sized datasets. Consolidated the entire data lifecycle into a **unified, configuration-driven (YAML) pipeline engine** that enforced reproducibility across the team."
          - "Engineered advanced data processors within this engine for **semantic deduplication** (using embeddings), **automated quality analysis**, and seamless format conversions (Text/SFT/DPO/RLHF and more). This internal infrastructure was later modularized and open-sourced as **DatasetPipeline**."
          - "Solved the 'Context Window Limit' for tool usage by implementing a **'RAG for Tools'** system, using vector search to dynamically retrieve and inject relevant Python function definitions at runtime."
          - "Built a 'Probabilistic-to-Deterministic' bridge using **Pydantic V2** to enforce strict schema validation on LLM outputs, acting as a firewall against hallucinations."
      - name: "CV Advisors - High-Performance Financial Data Processing"
        date: "Jan 2024 – Jan 2024"
        show_on_resume: false
        highlights:
          - "Executed a high-stakes performance engineering PoC to replace a legacy 1900+ line SQL stored procedure that suffered from 'Row-By-Agonizing-Row' (RBAR) processing."
          - "Identified the critical bottleneck: nested `WHILE` loops and a scalar UDF (`dbo.ufn_CalcPnL`) that prevented SQL parallelism. Re-architected the solution to eliminate these blocking factors entirely."
          - "Designed a hybrid compute engine using **DuckDB for high-speed set aggregations** and **Pandas for vectorized window operations** (Cumulative Sums/Shifts), utilizing the best tool for each mathematical operation."
          - "Reduced runtime from 27.5 hours to under 5 seconds (~20,000x faster) for 80M+ records across 150 clients, shattering the client's 5-minute target via pure in-memory computation."
          - "Engineered a **future-proof architecture** capable of scaling beyond RAM limits via memory-optimized categorical typing (reducing footprint by ~4.5x) and DuckDB’s out-of-core processing capabilities."
          - "Ensured **100% numerical accuracy to 8 decimal places** against the legacy production system, validating the financial integrity of the new pipeline before delivery."
          - "Implemented complex business logic for 'Phantom Accounts' (aggregating child entities) and 'Subtype History', transforming a monolithic SQL script into modular, testable Object-Oriented Python."
          - "Built a smart local Parquet caching layer to eliminate network I/O latency during development and re-runs, significantly accelerating the debugging cycle."
      - name: "LoopKitchen (now Loop) - Food Delivery Intelligence Platform"
        date: "Sep 2022 – Jun 2023"
        show_on_resume: false
        highlights:
          - "Built core data architecture from scratch for a food delivery analytics startup using a comprehensive GCP stack: FastAPI/SQLModel on Cloud Run, BigQuery for warehousing, Firestore for metadata tracking and orchestrated via GCP Workflows."
          - "Developed multi-platform data ingestion pipelines scraping and processing millions of orders from UberEats, DoorDash, and Grubhub APIs, enabling real-time performance analytics for restaurant brands and franchises."
          - "Engineered an automated dispute resolution system that allowed restaurants to automatically submit claims for bad orders, directly recovering lost revenue."
          - "Created a custom Streamlit monitoring dashboard providing real-time visibility into orchestration workflows with granular tracking (Brand→Region→Chain→Store→Order), essential for debugging distributed systems."
          - "Implemented sophisticated orchestration via GCP Workflows and established CI/CD for BigQuery routines, ensuring version-controlled and reproducible data infrastructure."
          - "Contributed as core team member (10-person startup) working directly with technical leadership, helping build the foundation for a platform that secured $6M Series A funding and now serves major restaurant chains like Dave's Hot Chicken and Freddy's."
      - name: "Logical Contract - AI-Powered Contract Generator"
        date: "Oct 2023 — Dec 2023"
        show_on_resume: false
        highlights:
          - "Implemented an AI-powered system to generate tailored employment agreements for startups based on dynamic user inputs."
          - "Developed a RAG-based legal chatbot that catered to legal inquiries, offering tailored responses based on verified legal context."
      - name: "SlideNinja - Enterprise AI Presentation Platform"
        date: "Jul 2023 — Oct 2023"
        show_on_resume: false
        highlights:
          - "Architected an enterprise-grade GenAI platform using a microservices-ready architecture, containerized via Docker and deployed on Google Cloud Run for serverless auto-scaling."
          - "Engineered a proprietary 'Geometric Layout Analysis' engine that calculates bounding boxes and shape overlaps to intelligently map AI-generated content into rigid corporate templates, overcoming the limitations of standard placeholder logic."
          - "Implemented a 'Smart Template' parser that reads configuration data directly from a hidden slide within the PPTX file, allowing dynamic template updates without code changes."
          - "Built a 'Self-Healing' AI orchestration layer with a recursive rectification loop: if the LLM generates malformed JSON, the system automatically feeds the error trace back to the model for correction, ensuring robust production reliability."
          - "Designed a high-concurrency backend using FastAPI (AsyncIO) and ThreadPoolExecutor to 'fan-out' slide generation requests, reducing deck creation time by ~80% via Map-Reduce parallelism."
          - "Solved serverless state issues by building a stateless template synchronization engine that uses MD5 hashing to sync local ephemeral storage with Google Cloud Storage on startup."
          - "Developed low-level XML manipulation routines to handle complex typography features (dynamic text fitting, custom bullet styling, contrast-aware font coloring) not supported by standard libraries."
          - "Integrated a secure RBAC system with MongoDB Atlas for user state and request logging, implementing a custom `@cache_it` decorator to minimize API costs through intelligent prompt caching."
      - name: "Eningo - Automated Cable & Utility Mapping Platform"
        date: "Apr 2022 — Aug 2022"
        show_on_resume: false
        highlights:
          - "Developed a cloud-based platform for automating cable network issue requests and processing geospatial data."
          - "Migrated legacy data from Postgres to MongoDB, enhancing scalability and efficiency."
      - name: "NIBE - DevOps Automation for Sustainable Energy Tech"
        date: "Feb 2022 — Mar 2022"
        show_on_resume: false
        highlights:
          - "Created and managed CI/CD pipelines on Azure DevOps, impacting over 50k daily users."

    personal_projects:
      # Resume + Website projects
      - name: "SQLStream"
        date: "Nov 2025"
        show_on_resume: true
        highlights:
          - "Created a **zero-setup SQL tool** to quickly analyze data files (CSV, Parquet, Markdown, HTML) via CLI or Python, streamlining repetitive data quality checks without DB overhead."
          - "Built a powerful interactive shell with syntax highlighting and history for data exploration, alongside a standard query mode for piping results into other terminal tools."
          - "Engineered a flexible engine that auto-switches between **DuckDB**, Pandas, or native Python to execute queries; published on [PyPI](https://pypi.org/project/sqlstream) (**3.4k+ downloads**) with full [documentation](https://subhayu99.github.io/sqlstream)."

      - name: "Smart Commit"
        date: "May 2025"
        show_on_resume: true
        highlights:
          - "Built an AI-powered CLI tool using Python/Typer that generates context-aware git commits via OpenAI or Anthropic models, enhancing developer productivity."
          - "Intelligently adapts to different projects by analyzing the existing tech stack, commit history, and file changes, ensuring contextually relevant messages."
          - "Published on [PyPI](https://pypi.org/project/smart-commit-ai) (**5k+ downloads**) and [GitHub](https://github.com/subhayu99/smart-commit) with **Model Context Protocol (MCP)** integration."

      - name: "DocumentAccessPOC"
        date: "Jan 2025"
        show_on_resume: true
        highlights:
          - "Designed and **built a zero-trust secure document system** to solve granular access control challenges where traditional RBAC/ACLs fail."
          - "Implemented a robust cryptographic model featuring **end-to-end encryption (AES-GCM)** and secure key exchange (RSA) to enforce permissions at a data level."
          - "Built a FastAPI interface for secure document sharing and revocation; project is on [GitHub](https://github.com/subhayu99/DocumentAccessPOC) with detailed [documentation](https://subhayu99.github.io/DocumentAccessPOC/)."

      # Website-only personal projects
      - name: "DatasetPipeline"
        date: "May 2025"
        show_on_resume: false  # <--- FALSE because it's already in QxLab Experience
        highlights:
          - "Developed a **production-ready CLI tool** for transforming messy datasets into ML-ready formats; supports **SFT, DPO, semantic deduplication**, and quality analysis."
          - "Features smart role mapping, auto-formatting for OpenAI-style training, and reproducible workflows via YAML/JSON configuration for enterprise ML pipelines."
          - "Published on [PyPI](https://pypi.org/project/datasetpipeline) (**6.7k+ downloads**) and open-sourced on [GitHub](https://github.com/subhayu99/datasetpipeline) with extensible architecture for custom loaders, formatters, and analyzers."

      - name: "creatree"
        date: "Feb 2025"
        show_on_resume: false
        highlights:
          - "Developed a Python CLI tool and library for automating directory structure creation from tree-like strings, solving repetitive project scaffolding."
          - "Designed for maximum flexibility with both a Python library for programmatic integration and a CLI supporting stdin/pipes."
          - "Published on [PyPI](https://pypi.org/project/creatree) (**4.3k+ downloads**) and open-sourced on [GitHub](https://github.com/subhayu99/creatree)."

      - name: "BetterPassphrase"
        date: "Jan 2024"
        show_on_resume: false
        highlights:
          - "Built a Python CLI tool/library for generating secure, memorable passphrases using grammatically correct phrases with customizable word counts."
          - "Implemented probabilistic security analysis with entropy calculations and parts-of-speech parsing to balance memorability with cryptographic strength."
          - "Published on [PyPI](https://pypi.org/project/BetterPassphrase) (**7.6k+ downloads**) and open-sourced on [GitHub](https://github.com/subhayu99/BetterPassphrase) supporting batch generation."
      - name: "FINADICT - Financial Prediction App"
        date: "Sep 2021"
        show_on_resume: false
        highlights:
          - "Built an interactive financial forecasting tool with Streamlit, enabling users to analyze and predict market data for stocks, forex, and cryptocurrencies."
          - "Implemented a suite of analytical features, including price forecasting via FB Prophet, interactive Plotly visualizations, and downloadable data reports."
          - "Managed the project's full lifecycle from development to deployment, containerizing the app with Docker and maintaining it as a live, public-facing service. Now open-sourced on [GitHub](https://github.com/subhayu99/finadict)."

    publication:
      - title: "QuDiet: A Classical Simulation Platform for Qubit-Qudit Hybrid Quantum Systems"
        authors:
          - "Subhayu Kumar Bala"
          - "Turbasu Chatterjee"
          - "Arnav Das"
        date: "2023-03-28"
        journal: "IET Quantum Communication"
        doi: "10.1049/qtc2.12058"
design:
  theme: engineeringresumes
  page:
    size: a4
    top_margin: 1.05cm
    bottom_margin: 1.05cm
    left_margin: 1.05cm
    right_margin: 1.05cm
    show_page_numbering: false
    show_last_updated_date: false
  colors:
    text: "#011629"
    name: "#124d86"
    connections: "#124d86"
    section_titles: "#124d86"
    links: "#124d86"
    last_updated_date_and_page_numbering: "grey"
  text:
    # --- FONTS ---
    # ----- TOP TIER (Safe & Modern) -----
    # Inter: The "Silicon Valley Standard." Tall x-height makes it ultra-readable at small sizes (9pt). 
    #        Perfect for dense, keyword-heavy technical resumes. Clean, invisible, and professional.
    # Roboto: "Engineered & Geometric." Has straight-sided curves that feel mechanical and precise. 
    #         Excellent for data/infrastructure roles where you want to signal "structure" and "order."
    # Open Sans: The "Neutral Choice." Extremely legible, open shapes. It has zero "personality," 
    #            which is good if you want the recruiter to focus 100% on the content. Safest bet.

    # ----- SPACE SAVERS (If you need to fit more text) -----
    # Source Sans 3: Designed for UIs and coding. Slightly condensed width. 
    #                Use this if your resume is spilling onto a new page by just 2-3 lines.
    # Noto Sans: Similar to Open Sans but with better support for special characters and a slightly 
    #            tighter vertical rhythm. Good for maximizing vertical space.

    # ----- PERSONALITY (If the resume feels too "cold") -----
    # Lato: "Humanist Sans." Semi-rounded details give it a friendly, approachable vibe. 
    #       Use this if you are applying to startups or culture-focused roles to sound less "corporate."
    # Raleway: (Use for HEADERS ONLY). Elegant and artistic. Too wide/distracting for body text, 
    #          but makes your Name/Title look very high-end.

    # ----- ACADEMIC / TRADITIONAL (Use for Research/Senior Management) -----
    # Libertinus Serif: A modern take on Times New Roman. Looks very "Senior" and "Official."
    #                   Use this if applying to traditional banks, law firms, or PhD research roles.
    # EB Garamond: The "Classy" choice. Old-style serif. Looks like a book. High prestige, low "tech" vibe.
    font_family: Inter
    font_size: 9.6pt
    alignment: justified
    date_and_location_column_alignment: right
    leading: "0.65em"
  header:
    name_font_family: Inter
    name_font_size: 16pt
    name_bold: true
    small_caps_for_name: false
    photo_width: 3.5cm
    vertical_space_between_name_and_connections: 0.4cm
    vertical_space_between_connections_and_first_section: 0.4cm
    horizontal_space_between_connections: 0.25cm
    connections_font_family: Inter
    separator_between_connections: '|'
    use_icons_for_connections: false
    make_connections_links: true
    alignment: center
  section_titles:
    type: with-full-line
    font_family: Inter
    font_size: 1.3em
    bold: true
    small_caps: true
    vertical_space_above: 0.4cm
    vertical_space_below: 0.2cm
  entries:
    date_and_location_width: 4cm
    left_and_right_margin: 0cm
    horizontal_space_between_columns: 0.1cm
    vertical_space_between_entries: 1em
    allow_page_break_in_sections: true
    allow_page_break_in_entries: true
    short_second_row: false
    show_time_spans_in: []
  highlights:
    bullet: •
    nested_bullet: ◦
    top_margin: 0.22cm
    left_margin: 0.3cm
    vertical_space_between_highlights: 0.22cm
    horizontal_space_between_bullet_and_highlight: 0.4em
    summary_left_margin: 0.2cm
  entry_types:
    one_line_entry:
      template: '**LABEL:** DETAILS'
    education_entry:
      main_column_first_row_template: |-
        **INSTITUTION**
        DEGREE in AREA
      degree_column_template: ''
      degree_column_width: 3cm
      main_column_second_row_template: |-
        SUMMARY
        HIGHLIGHTS
      date_and_location_column_template: |-
        LOCATION
        DATE
    normal_entry:
      main_column_first_row_template: '**NAME**'
      main_column_second_row_template: |-
        SUMMARY
        HIGHLIGHTS
      date_and_location_column_template: |-
        LOCATION
        DATE
    experience_entry:
      main_column_first_row_template: |-
        **COMPANY**
        POSITION
      main_column_second_row_template: |-
        SUMMARY
        HIGHLIGHTS
      date_and_location_column_template: |-
        LOCATION
        DATE
    publication_entry:
      main_column_first_row_template: "**TITLE**"
      main_column_second_row_template: |-
        AUTHORS
        URL (JOURNAL)
      main_column_second_row_without_journal_template: |-
        AUTHORS
        URL
      main_column_second_row_without_url_template: |-
        AUTHORS
        JOURNAL
      date_and_location_column_template: "DATE"
locale:
  language: "en"
  phone_number_format: international
  page_numbering_template: "NAME - Page PAGE_NUMBER of TOTAL_PAGES"
  last_updated_date_template: "Last updated in TODAY"
  date_template: "MONTH_ABBREVIATION YEAR"
  month: "month"
  months: "months"
  year: "year"
  years: "years"
  present: "Present"
  to: "-"
  abbreviations_for_months:
    - "Jan"
    - "Feb"
    - "Mar"
    - "Apr"
    - "May"
    - "Jun"
    - "Jul"
    - "Aug"
    - "Sep"
    - "Oct"
    - "Nov"
    - "Dec"
  full_names_of_months:
    - "January"
    - "February"
    - "March"
    - "April"
    - "May"
    - "June"
    - "July"
    - "August"
    - "September"
    - "October"
    - "November"
    - "December"
